{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# open dataset file\n",
    "dataset_path = \"./dataset/gsj_ac_dataset.pickle\"\n",
    "with open(dataset_path, \"rb\") as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you'll find some light dataset documentation.\n",
    "\n",
    "```\n",
    "{\n",
    "    id: {\n",
    "        \"expected\": # Target/working word, paired with \"label\". \n",
    "                    # Contains word form and phoneme translation\n",
    "        \"label\":    # Ground truth pronunciation label (True/False)\n",
    "        \"metadata\": # Important metadata such as \n",
    "                    # activityId, storyId, phraseIndex, and word_index\n",
    "        \"phrase\":   # Phrase text of which the expected word is a part of.\n",
    "                    # Contains word and phoneme forms.\n",
    "        \"asr\":      # Contains what I think is the most important ASR data \n",
    "                    # for a potential automated mispronunciation system.\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['expected', 'label', 'metadata', 'phrase', 'asr'])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "sample = dataset[random.randint(0,len(dataset))]\n",
    "print(sample.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, \"expected\" and \"label\":\n",
    "\n",
    "```\n",
    "{\n",
    "    id: {\n",
    "        \"expected\": {\n",
    "            \"word\":     # Target/working word as given in the labels.csv file\n",
    "            \"phoneme\":  # Target/working word translated to AMIRABET phonemes\n",
    "        }\n",
    "        \"label\": 0/1    # Whether the reader pronounced the expected word correctly\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word': 'take', 'phoneme': 'tak'}\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(sample[\"expected\"])\n",
    "print(sample[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important metadata for diagnosing problems and coordinating between data sources.\n",
    "\n",
    "```\n",
    "{\n",
    "    id: {\n",
    "        \"metadata\": {\n",
    "            \"activityId\":   # Id corresponding to the reading session\n",
    "            \"storyId\":      # Id corresponding to overall story being read\n",
    "            \"phraseIndex\":  # Index of current phrase in story being read\n",
    "            \"word_index\":   # Index of current word in phrase\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activityId': 'FC1D174030EB11EC89641635D148', 'storyId': '4B5718806EB011EABBC087B56D5C6D4A', 'phraseIndex': 5, 'word_index': 3}\n"
     ]
    }
   ],
   "source": [
    "print(sample[\"metadata\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phrase data, in word and phoneme form. Each has been tokenized.\n",
    "\n",
    "```\n",
    "{\n",
    "    id: {\n",
    "        \"phrase\": {\n",
    "            \"word\":     # Current phrase in word form\n",
    "            \"phoneme\":  # Current phrase in phoneme form\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word': ['val', 'helps', 'casey', 'take', 'care', 'of', 'the', 'lamb'], 'phoneme': ['væl', 'hɛlps', 'kasi', 'tak', 'kɛɹ', 'ʌv', 'θʌ', 'læm']}\n"
     ]
    }
   ],
   "source": [
    "print(sample[\"phrase\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASR data attributes which I think are of highest value, prepared in the most flexible way possible. This includes tokenized word and phoneme form as well as confidence data depending on ASR source.\n",
    "\n",
    "```\n",
    "{\n",
    "    id: {\n",
    "        \"asr\": {\n",
    "            \"amazon_data\":                  # word, word_confidence, and phoneme\n",
    "            \"kaldi_data\":                   # word, word_confidence, and phoneme\n",
    "            \"kaldiNa_data\":                 # word, word_confidence, and phoneme\n",
    "            \"wav2vec_transcript_words\":     # word and phoneme\n",
    "            \"wav2vec_transcript_phonemes\":  # phoneme\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['amazon_data', 'kaldi_data', 'kaldiNa_data', 'wav2vec_transcript_words', 'wav2vec_transcript_phonemes'])\n",
      "\n",
      "amazon:  {'word': ['fell', 'helps', 'casey', 'take', 'car', 'of', 'the', 'lap', 'first', 'casey'], 'word_confidence': (0.3942, 1, 0.8517, 1, 0.8232, 0.2946, 1, 0.3627, 1, 0.7284), 'phoneme': ['fɛl', 'hɛlps', 'kasi', 'tak', 'kɑɹ', 'ʌv', 'θʌ', 'læp', 'fɝst', 'kasi']}\n",
      "\n",
      "kaldi:  {'word': ['val', 'helps', 'casey', 'take', 'car', 'of', 'the', 'lamb'], 'word_confidence': [0.6368449330329895, 1.0, 0.781516432762146, 0.9803750514984131, 0.46812132000923157, 1.0, 0.9921054840087891, 0.7626157999038696], 'phoneme': ['væl', 'hɛlps', 'kasi', 'tak', 'kɑɹ', 'ʌv', 'θʌ', 'læm']}\n",
      "\n",
      "kaldiNa:  {'word': ['val', 'helps', 'casey', 'take', 'care', 'of', 'the', '<UNK>'], 'word_confidence': [0.6485823392868042, 1.0, 1.0, 1.0, 0.9791126847267151, 1.0, 0.968928337097168, 0.6780429482460022], 'phoneme': ['væl', 'hɛlps', 'kasi', 'tak', 'kɛɹ', 'ʌv', 'θʌ', False]}\n",
      "\n",
      "wave2vec_word:  {'word': ['bell', 'helps', 'casey', 'take', 'care', 'of', 'the', 'lam'], 'phoneme': ['bɛl', 'hɛlps', 'kasi', 'tak', 'kɛɹ', 'ʌv', 'θʌ', 'læm']}\n",
      "\n",
      "wave2vec_phoneme:  {'phoneme': ['bɪl', 'hɛlps', 'kasi', 'tak', 'kɛɹ', 'ʌv', 'θʌ']}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sample[\"asr\"].keys())\n",
    "print()\n",
    "print(\"amazon: \", sample[\"asr\"][\"amazon_data\"])\n",
    "print()\n",
    "print(\"kaldi: \", sample[\"asr\"][\"kaldi_data\"])\n",
    "print()\n",
    "print(\"kaldiNa: \", sample[\"asr\"][\"kaldiNa_data\"])\n",
    "print()\n",
    "print(\"wave2vec_word: \", sample[\"asr\"][\"wav2vec_transcript_words\"])\n",
    "print()\n",
    "print(\"wave2vec_phoneme: \", sample[\"asr\"][\"wav2vec_transcript_phonemes\"])\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsj-ac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
